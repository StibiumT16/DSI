{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364e00e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2397666",
   "metadata": {},
   "source": [
    "## Origina data transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dad35605",
   "metadata": {},
   "source": [
    "###### Download NQ Train and Dev dataset from https://ai.google.com/research/NaturalQuestions/download\n",
    "###### NQ Train: https://storage.cloud.google.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz\n",
    "###### NQ Dev: https://storage.cloud.google.com/natural_questions/v1.0-simplified/nq-dev-all.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca748ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7830it [02:34, 50.81it/s]\n"
     ]
    }
   ],
   "source": [
    "nq_dev = []\n",
    "\n",
    "with gzip.open(\"v1.0-simplified_nq-dev-all.jsonl.gz\", \"r+\") as f:\n",
    "    for item in tqdm(jsonlines.Reader(f)):\n",
    "        \n",
    "        arr = []\n",
    "        ## question_text\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        tokens = []\n",
    "        for i in item['document_tokens']:\n",
    "            tokens.append(i['token'])\n",
    "        document_text = ' '.join(tokens)\n",
    "        \n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "\n",
    "        # document_text = item['document_text']\n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## url\n",
    "        arr.append(item['document_url'])\n",
    "        \n",
    "        ## title\n",
    "        arr.append(item['document_title'])\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "        doc_tac = item['document_title'] + abs + content\n",
    "        \n",
    "        arr.append(doc_tac)\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_dev.append(arr)\n",
    "\n",
    "nq_dev = pd.DataFrame(nq_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a93d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307373it [19:11, 266.87it/s]\n"
     ]
    }
   ],
   "source": [
    "nq_train = []\n",
    "with gzip.open(\"v1.0-simplified_simplified-nq-train.jsonl.gz\", \"r+\") as f:\n",
    "    for item in tqdm(jsonlines.Reader(f)):\n",
    "        ## question_text\n",
    "        arr = []\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "        \n",
    "        document_text = item['document_text']\n",
    "        \n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## url\n",
    "        arr.append(item['document_url'])\n",
    "        \n",
    "        ## title\n",
    "        if document_text.find('<H1>') != -1:\n",
    "            title_start = document_text.index('<H1>')\n",
    "            title_end = document_text.index('</H1>')\n",
    "            title = document_text[title_start+4:title_end]\n",
    "        else:\n",
    "            title = ''\n",
    "        arr.append(title)\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "\n",
    "        doc_tac = title + abs + content\n",
    "        \n",
    "        arr.append(doc_tac)\n",
    "\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_train.append(arr)\n",
    "\n",
    "nq_train = pd.DataFrame(nq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c45cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109739"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def lower(x):\n",
    "    text = tokenizer.tokenize(x)\n",
    "    id_ = tokenizer.convert_tokens_to_ids(text)\n",
    "    return tokenizer.decode(id_)\n",
    "\n",
    "nq_dev['title'] = nq_dev['title'].map(lower)\n",
    "nq_train['title'] = nq_train['title'].map(lower)\n",
    "\n",
    "nq_all_doc = nq_train.append(nq_dev)\n",
    "nq_all_doc.reset_index(inplace = True)\n",
    "nq_all_doc.drop_duplicates('title', inplace = True)\n",
    "nq_all_doc.reset_index(inplace = True)\n",
    "len(nq_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6146320",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_doc = {}\n",
    "title_doc_id = {}\n",
    "id_doc = {}\n",
    "id_url = {}\n",
    "id_title = {}\n",
    "ran_id_old_id = {}\n",
    "idx = 0\n",
    "for i in range(len(nq_all_doc)):\n",
    "    title_doc[nq_all_doc['title'][i]] =  nq_all_doc['doc_tac'][i]\n",
    "    title_doc_id[nq_all_doc['title'][i]] = idx\n",
    "    id_url[idx] = nq_all_doc['url'][i]\n",
    "    id_doc[idx] = nq_all_doc['doc_tac'][i]\n",
    "    id_title[idx] = nq_all_doc['title'][i]\n",
    "    ran_id_old_id[idx] = nq_all_doc['id'][i]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a461690",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/corpus.json', 'w') as fw:\n",
    "    for docid in id_doc.keys():\n",
    "        fw.write(json.dumps({\"docid\": str(docid), \"url\" : id_url[docid].replace(\"&amp\", \"\"), \"title\" : id_title[docid], \"body\" : \" \".join(id_doc[docid].split()[:512])}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfb3f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307373\n",
      "7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307373/307373 [00:04<00:00, 73133.60it/s]\n",
      "100%|██████████| 7830/7830 [00:00<00:00, 78363.22it/s]\n"
     ]
    }
   ],
   "source": [
    "print(len(nq_train))\n",
    "print(len(nq_dev))\n",
    "with open(\"dataset/nq-doctrain-qrels.tsv\", \"w\") as fw1, open(\"dataset/nq-doctrain-queries.tsv\", \"w\") as fw2:\n",
    "    for i in tqdm(range(len(nq_train))):\n",
    "        fw2.write(str(i) + \"\\t\" + nq_train['query'][i] + \"\\n\")\n",
    "        fw1.write(str(i) + \"\\t0\\t\" + str(title_doc_id[nq_train['title'][i]]) + \"\\t1\\n\")\n",
    "\n",
    "with open(\"dataset/nq-docdev-qrels.tsv\", \"w\") as fw1, open(\"dataset/nq-docdev-queries.tsv\", \"w\") as fw2:\n",
    "    for i in tqdm(range(len(nq_dev))):\n",
    "        fw2.write(str(i) + \"\\t\" + nq_dev['query'][i] + \"\\n\")\n",
    "        fw1.write(str(i) + \"\\t0\\t\" + str(title_doc_id[nq_dev['title'][i]]) + \"\\t1\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
